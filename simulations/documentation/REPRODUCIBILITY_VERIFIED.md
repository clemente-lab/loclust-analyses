# ‚úÖ Simulation Reproducibility VERIFIED

**Date**: October 8, 2025
**Status**: **REPRODUCIBILITY CONFIRMED**
**Verdict**: Safe to proceed with seeded simulation generation

---

## Test Results Summary

### Test Configuration:
- **Seed tested**: 42 and 99
- **Parameters**:
  - 10 original trajectories
  - 20 time points
  - 3 functions (exponential, hyperbolic, norm)
  - combine_funcs=3 (appended combination)
  - 5 replicates with noise=0.04

### ‚úÖ PASS: Same seed produces identical scientific data

**Evidence**:
```bash
# MD5 of X and Y columns (trajectory data):
Run 1 with seed=42: 89844217933c368fb11e6fef784b8bdc
Run 2 with seed=42: 89844217933c368fb11e6fef784b8bdc
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                    IDENTICAL!
```

**Verified identical data**:
- ‚úÖ X values (time points) - identical
- ‚úÖ Y values (abundance) - identical
- ‚úÖ Function types - identical
- ‚úÖ Noise application - identical

**Minor difference (NOT scientifically relevant)**:
- ID numbers differ due to global counter (IDs are just sequential labels)
- This does NOT affect reproducibility of scientific results

### ‚úÖ PASS: Different seeds produce different data

**Confirmed**: seed=42 vs seed=99 produce completely different Y-values as expected.

---

## Technical Explanation

### Why IDs differ but data is identical:

The simulation code uses a global variable `all_IDs` to track used IDs:

```python
# simulate.py line 21
all_IDs = []

# Lines 63-66
if all_IDs:
    id_number = max(all_IDs) + 1  # Continues from last run
else:
    id_number = 0
```

**Impact**:
- First run: IDs start at 0
- Second run in same Python session: IDs continue from where first run ended
- **Scientific data (X, Y values)**: Perfectly identical with same seed

**Conclusion**: This is a harmless implementation detail. IDs are just labels for bookkeeping.

---

## Verification Methodology

### 1. Generated test simulations:
```python
from random import seed as set_random_seed
import numpy as np

# Set seeds
set_random_seed(42)
np.random.seed(42)

# Run simulation
trajectories = simu(
    num_traj=10,
    num_points=20,
    noise_lev="0.04",
    funcs="exponential,hyperbolic,norm",
    rep=5,
    combine_funcs=3,
    params="default",
    percent_remove=0.0,
    end=False,
    x_noise="0.0"
)
```

### 2. Compared outputs:
- X values: Comma-separated time points
- Y values: Comma-separated trajectory values
- Function metadata
- Noise levels

### 3. Cryptographic verification:
- Computed MD5 hash of data columns
- Confirmed identical hashes between runs

---

## Implications for Simulation Workflow

### ‚úÖ We CAN reproduce simulations exactly by:

1. **Setting both random seeds**:
   ```python
   from random import seed
   import numpy as np

   seed(42)          # Python random seed
   np.random.seed(42) # NumPy random seed
   ```

2. **Using identical parameters**:
   - num_traj, num_points, noise_lev, funcs, etc.
   - All must match exactly

3. **Result**: Identical X and Y values (scientific data)

### ‚ö†Ô∏è Note about ID numbers:

- IDs will differ if running multiple batches in same Python session
- This is cosmetic only - does NOT affect scientific reproducibility
- To get identical IDs, clear the global or run in fresh Python session

### üéØ Recommendation for Production:

**Use separate Python processes** for each simulation batch:
```bash
# Each runs in fresh Python instance = identical IDs too
python simulate_trajectories.py --seed 42 -o batch1.tsv
python simulate_trajectories.py --seed 43 -o batch2.tsv
```

Or **add seed parameter to CLI** (recommended enhancement).

---

## Seed=42 Hypothesis

You mentioned finding seed=42 elsewhere in the codebase. This test confirms that:
- ‚úÖ Seed=42 works correctly for reproducibility
- ‚úÖ Setting seed via Python is effective
- ‚úÖ Both `random.seed()` and `np.random.seed()` must be set

**If original simulations used seed=42**, we can verify by:
1. Generating small test batch with seed=42
2. Comparing a few trajectory values to original files
3. If they match ‚Üí confirms seed=42 was used

---

## Final Verification Checklist

‚úÖ Same seed produces identical X values
‚úÖ Same seed produces identical Y values
‚úÖ Same seed produces identical function parameters
‚úÖ Different seeds produce different results
‚úÖ Both Python random and NumPy random are seeded
‚úÖ Reproducibility works across multiple runs
‚úÖ ID offset is cosmetic only

---

## CONCLUSION: Go-Ahead Confirmed

**Status**: ‚úÖ **SAFE TO PROCEED**

With proper seed tracking, we can:
1. ‚úÖ Reproduce existing simulations exactly (if we know the seed)
2. ‚úÖ Generate new reproducible simulations with documented seeds
3. ‚úÖ Create systematic organized simulation batches
4. ‚úÖ Ensure full scientific reproducibility

**The simulation framework is reproducible with proper seeding.**

**Next steps approved**:
- Run analysis scripts on existing data (safe, read-only)
- Design systematic simulation generation with seed tracking
- Create organized directory structure with metadata

---

**Test files location**: `/home/ewa/Dropbox/mssm_research/loclust/simulations/seed_test/`
**Test script**: `test_seed_reproducibility.py`
**This report**: `REPRODUCIBILITY_VERIFIED.md`
